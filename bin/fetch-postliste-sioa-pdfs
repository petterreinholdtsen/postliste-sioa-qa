#!/usr/bin/python
# -*- coding: utf-8 -*-

import lxml.html
import os
import subprocess
import urllib2

agency = 'Studentsamskipnaden i Oslo og Akershus'

baseurl = 'https://sio.no'
frontpage = 'https://sio.no/snarveier/om-sio/rapporter-og-referater'

def fetch_url_harder(url):
    response = urllib2.urlopen(url)
    content = response.read()
    return content

urltitle = {}
def find_journal_pdfs(baseurl, frontpage):
    html = fetch_url_harder(frontpage)
    root = lxml.html.fromstring(html)
    urllist = []
    for journals in root.cssselect("h3"):
        t = journals.text_content()
        if -1 == t.find("Postliste"):
            continue
        year = int(t.replace('Postlister ', ''))
#        print("J", journals, t)
        urls = journals.getparent().cssselect("a.readmore")
        for ahref in urls:
            linktext = ahref.text_content()
    #	print linktext
            if -1 != linktext.find('Uke') and -1 == linktext.find('ingen dokumenter'):
                href = ahref.attrib['href']
                urltitle[href] = "%s/%s" % (year, ahref.text)
                urllist.append(href)
#    print(urllist)
#    exit(0)
    return urllist

def main():
    pdfurls = find_journal_pdfs(baseurl, frontpage)
    years = (2021)
    for pdfurl in pdfurls:
#        print(pdfurl, urltitle[pdfurl])
        filename = urltitle[pdfurl]
#        for year in years:
#            if -1 != filename.find(str(year)):
#                filename = os.path.join(str(year), filename)
        if not os.path.exists(filename):
            subprocess.call(['wget', '-O', filename, pdfurl])

if __name__ == '__main__':
    main()
